{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBABnTZKncKU",
        "outputId": "0651dc70-6f00-4a17-c03e-9fa9c3e28523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl -f https://data.dgl.ai/wheels/repo.html\n",
        "\n",
        "!pip install dglgo -f https://data.dgl.ai/wheels-test/repo.html"
      ],
      "metadata": {
        "id": "svYZdSBIDaAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "spueVRy90Smx"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import dgl.function as fn\n",
        "import torch.nn.functional as F\n",
        "import shutil\n",
        "from torch.utils.data import DataLoader\n",
        "import cloudpickle\n",
        "from dgl.nn import GraphConv\n",
        "from sklearn import preprocessing\n",
        "import math\n",
        "from dgl.nn import SAGEConv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUz_GYH70Smy"
      },
      "source": [
        "#### Set Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qxEuIXHK0Smy"
      },
      "outputs": [],
      "source": [
        "current_dir = \"/content/drive/MyDrive/GML Final Project/\"\n",
        "checkpoint_path = current_dir + \"save_models/model_checkpoints/\" + \"checkpoint\"\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "best_model_path = current_dir + \"save_models/best_model/\"\n",
        "\n",
        "folder_data_temp = current_dir +\"data_temp/\"\n",
        "shutil.rmtree(folder_data_temp, ignore_errors=True)\n",
        "\n",
        "path_save = current_dir + \"Lipophilicity.zip\"\n",
        "shutil.unpack_archive(path_save, folder_data_temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AGL3B0D0Smy"
      },
      "source": [
        "#### Custom PyTorch Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vMxO0rAv0Smy"
      },
      "outputs": [],
      "source": [
        "\"\"\" Regression Dataset \"\"\"\n",
        "class DGLDatasetReg(torch.utils.data.Dataset):\n",
        "    def __init__(self, address, transform=None, train=False, scaler=None, scaler_regression=None):\n",
        "      #The train attribute is set to the value of the train argument, indicating if the dataset is for training or not.\n",
        "            self.train = train\n",
        "            self.scaler = scaler\n",
        "      #The data_set attribute is assigned the loaded graphs from the specified address using dgl.load_graphs.\n",
        "            self.data_set, train_labels_masks_globals = dgl.load_graphs(address+\".bin\")\n",
        "            num_graphs = len(self.data_set)\n",
        "      #The labels, masks, and globals are extracted from train_labels_masks_globals obtained from loading the graphs.\n",
        "      #They are reshaped into 2-dimensional tensors, where each row corresponds to the labels, masks, or globals of a graph.\n",
        "            self.labels = train_labels_masks_globals[\"labels\"].view(num_graphs,-1)\n",
        "            self.masks = train_labels_masks_globals[\"masks\"].view(num_graphs,-1)\n",
        "            self.globals = train_labels_masks_globals[\"globals\"].view(num_graphs,-1)\n",
        "      #The transform attribute is assigned the value of the transform argument, which can be used for data transformation if provided.\n",
        "            self.transform = transform\n",
        "      #The scaler_regression attribute is assigned the value of the scaler_regression argument, indicating whether a scaler should be used for regression labels or not.\n",
        "            self.scaler_regression = scaler_regression\n",
        "#The scaler_method method is defined to create and return a scaler object. If self.\n",
        "#train is True, it fits a StandardScaler to the labels and assigns it to the self.scaler attribute. The scaler is returned.\n",
        "    def scaler_method(self):\n",
        "        if self.train:\n",
        "            scaler = preprocessing.StandardScaler().fit(self.labels)\n",
        "            self.scaler = scaler\n",
        "        return self.scaler\n",
        "#The __len__ method returns the number of graphs in the dataset.\n",
        "    def __len__(self):\n",
        "        return len(self.data_set)\n",
        "#The __getitem__ method retrieves an item from the dataset at the given index (idx). There are two different paths based on whether self.scaler_regression is True or not:\n",
        "    def __getitem__(self, idx):\n",
        "        if self.scaler_regression:\n",
        "            \"\"\" With Scaler\"\"\"\n",
        "            return  self.data_set[idx], torch.tensor(self.scaler.transform(self.labels)[idx]).float(), self.masks[idx], self.globals[idx]\n",
        "        else:\n",
        "            \"\"\" Without Scaler \"\"\"\n",
        "            return  self.data_set[idx], self.labels[idx].float(), self.masks[idx], self.globals[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwPHz3jO0Smz"
      },
      "source": [
        "#### Defining Train, Validation, and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPMBUvb80Smz",
        "outputId": "4c1a86a9-dc4e-44f6-e38a-473558d98518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3360 420 420\n"
          ]
        }
      ],
      "source": [
        "scaler = {}\n",
        "path_data_temp = folder_data_temp + \"scaffold\"+\"_\"+str(0)\n",
        "train_set = DGLDatasetReg(address=path_data_temp+\"_train\", train=True)\n",
        "scaler = train_set.scaler_method()\n",
        "val_set = DGLDatasetReg(address=path_data_temp+\"_val\", scaler=scaler)\n",
        "test_set = DGLDatasetReg(address=path_data_temp+\"_test\", scaler=scaler)\n",
        "\n",
        "print(len(train_set), len(val_set), len(test_set))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0acizkE0Sm0"
      },
      "source": [
        "#### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gO6nHb_q0Sm0"
      },
      "outputs": [],
      "source": [
        "def collate(batch):\n",
        "    # batch is a list of tuples (graphs, labels, masks, globals)\n",
        "    # Concatenate a sequence of graphs\n",
        "\n",
        "    ### This line extracts the graphs from each tuple in the batch and creates a list of individual graphs.\n",
        "    graphs = [e[0] for e in batch]\n",
        "    ### This line uses the dgl.batch() function to combine the individual graphs into a single graph batch g\n",
        "    g = dgl.batch(graphs)\n",
        "\n",
        "    # Concatenate a sequence of tensors (labels) along a new dimension\n",
        "\n",
        "    ### This line extracts the labels from each tuple in the batch and creates a list of individual label tensors.\n",
        "    labels = [e[1] for e in batch]\n",
        "    ### This line uses torch.stack() to concatenate the individual label tensors along a new dimension (dimension 0) and create a single tensor labels representing the labels for the entire batch.\n",
        "    labels = torch.stack(labels, 0)\n",
        "\n",
        "    # Concatenate a sequence of tensors (masks) along a new dimension\n",
        "\n",
        "    ### This line extracts the masks from each tuple in the batch and creates a list of individual mask tensors.\n",
        "    masks = [e[2] for e in batch]\n",
        "    ### This line uses torch.stack() to concatenate the individual mask tensors along a new dimension (dimension 0) and create a single tensor masks representing the masks for the entire batch.\n",
        "    masks = torch.stack(masks, 0)\n",
        "\n",
        "    # Concatenate a sequence of tensors (globals) along a new dimension\n",
        "\n",
        "    ### This line extracts the global features from each tuple in the batch and creates a list of individual global feature tensors.\n",
        "    globals = [e[3] for e in batch]\n",
        "    ### This line uses torch.stack() to concatenate the individual global feature tensors along a new dimension (dimension 0) and create a single tensor globals representing the global features for the entire batch.\n",
        "    globals = torch.stack(globals, 0)\n",
        "\n",
        "    return g, labels, masks, globals\n",
        "\n",
        "\n",
        "def loader(batch_size=64):\n",
        "    train_dataloader = DataLoader(train_set,\n",
        "                              batch_size=batch_size,\n",
        "                              collate_fn=collate,\n",
        "                              drop_last=False,\n",
        "                              shuffle=True,\n",
        "                              num_workers=1)\n",
        "\n",
        "    val_dataloader =  DataLoader(val_set,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=collate,\n",
        "                             drop_last=False,\n",
        "                             shuffle=False,\n",
        "                             num_workers=1)\n",
        "\n",
        "    test_dataloader = DataLoader(test_set,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=collate,\n",
        "                             drop_last=False,\n",
        "                             shuffle=False,\n",
        "                             num_workers=1)\n",
        "    return train_dataloader, val_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k7oLkbLh0Sm1"
      },
      "outputs": [],
      "source": [
        "train_dataloader, val_dataloader, test_dataloader = loader(batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3VavJsl0Sm1"
      },
      "source": [
        "#### Defining A GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lk7pfCu0Sm1"
      },
      "source": [
        "##### Some Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PcuTUQry0Sm2"
      },
      "outputs": [],
      "source": [
        "#Bace dataset has 1 task. Some other datasets may have some more number of tasks, e.g., tox21 has 12 tasks.\n",
        "num_tasks = 1\n",
        "\n",
        "# Size of global feature of each graph\n",
        "global_size = 200\n",
        "\n",
        "# Number of epochs to train the model\n",
        "num_epochs = 100\n",
        "\n",
        "# Number of steps to wait if the model performance on the validation set does not improve\n",
        "patience = 10\n",
        "\n",
        "#Configurations to instantiate the model\n",
        "config = {\"node_feature_size\":127, \"edge_feature_size\":12, \"hidden_size\":100}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dgl.use_libxsmm(False)\n",
        "\n",
        "class CustomModule(nn.Module):\n",
        "    def __init__(self, in_feat, out_feat):\n",
        "        super(CustomModule, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(in_feat * 2, 256)\n",
        "        self.linear2 = nn.Linear(256, out_feat)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata[\"h\"] = h\n",
        "            g.update_all(\n",
        "                message_func=fn.copy_u('h', 'm'),\n",
        "                reduce_func=fn.max(\"m\", \"A\"),\n",
        "            )\n",
        "            A = g.ndata[\"A\"]\n",
        "            concat = torch.cat([h, A], dim=1)\n",
        "            output = self.linear1(concat)\n",
        "            output = self.relu(output)\n",
        "            output = self.linear2(output)\n",
        "            output = self.relu(output)\n",
        "            return output\n"
      ],
      "metadata": {
        "id": "T41nSqsq0Shv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-2cC2egfkVx"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size=200, num_tasks=1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        self.Custom1 = CustomModule(self.node_feature_size, self.hidden_size)\n",
        "        self.Custom2 = CustomModule(self.hidden_size, self.hidden_size)\n",
        "        self.Custom3 = CustomModule(self.hidden_size, self.hidden_size)\n",
        "        self.Custom4 = CustomModule(self.hidden_size, self.num_tasks)\n",
        "\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:, :self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:, :self.edge_feature_size]\n",
        "\n",
        "        h = mol_dgl_graph.ndata[\"v\"]\n",
        "        h = self.Custom1(mol_dgl_graph, h)\n",
        "        h = F.relu(h)\n",
        "        h = self.Custom2(mol_dgl_graph, h)\n",
        "        h = F.relu(h)\n",
        "        h = self.Custom3(mol_dgl_graph, h)\n",
        "        h = F.relu(h)\n",
        "        h = self.Custom4(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_score(model, data_loader, scaler, val_size, num_tasks=1):\n",
        "  model.eval()\n",
        "  loss_sum = nn.MSELoss(reduction='sum') # MSE with sum instead of mean, i.e., sum_i[(y_i)^2-(y'_i)^2]\n",
        "  final_loss = 0\n",
        "  state = torch.get_rng_state()\n",
        "  with torch.no_grad():\n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "       prediction = model(mol_dgl_graph, globals)\n",
        "       prediction = torch.tensor(scaler.inverse_transform(prediction.detach().cpu()))\n",
        "       labels = torch.tensor(scaler.inverse_transform(labels.cpu()))\n",
        "       loss = loss_sum(prediction, labels)\n",
        "       final_loss += loss.item()\n",
        "\n",
        "    final_loss /= val_size\n",
        "    final_loss = math.sqrt(final_loss)\n",
        "\n",
        "  return final_loss / num_tasks"
      ],
      "metadata": {
        "id": "gJe2woCfH4Jp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iyuF-2BiH4Jp"
      },
      "outputs": [],
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "    criterion = nn.MSELoss(reduction='none')\n",
        "    loss = mask*criterion(output,label)\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1pai-jJyH4Jp"
      },
      "outputs": [],
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train() # Prepare model for training\n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "B_5Qt4m8H4Jq"
      },
      "outputs": [],
      "source": [
        "def train_evaluate():\n",
        "\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "    best_val = 100000\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    while epoch <= num_epochs:\n",
        "        if patience_count <= patience:\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader,scaler, len(val_set), num_tasks)\n",
        "            if score_val < best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "\n",
        "    # best model save\n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "S5zCMmqMH4Jq"
      },
      "outputs": [],
      "source": [
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, scaler, len(test_set), num_tasks)\n",
        "\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574e8a47-2b23-4e41-befb-4f3b08be00a6",
        "id": "Zj7FagGRfkVx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 5.604 | Valid Score: 2.102\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 2.102 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 2/100 | Training Loss: 1.965 | Valid Score: 1.520\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 1.520 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 3/100 | Training Loss: 1.683 | Valid Score: 1.464\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 1.464 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 4/100 | Training Loss: 1.561 | Valid Score: 1.434\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 1.434 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 5/100 | Training Loss: 1.462 | Valid Score: 1.399\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 1.399 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 6/100 | Training Loss: 1.412 | Valid Score: 1.385\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 1.385 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 7/100 | Training Loss: 1.378 | Valid Score: 1.383\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 1.383 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 8/100 | Training Loss: 1.345 | Valid Score: 1.364\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 1.364 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 9/100 | Training Loss: 1.310 | Valid Score: 1.365\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 1.364 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 10/100 | Training Loss: 1.289 | Valid Score: 1.346\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 1.346 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 11/100 | Training Loss: 1.276 | Valid Score: 1.347\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 1.346 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 12/100 | Training Loss: 1.249 | Valid Score: 1.367\n",
            " \n",
            "Epoch: 12/100 | Best Valid Score Until Now: 1.346 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 13/100 | Training Loss: 1.245 | Valid Score: 1.333\n",
            " \n",
            "Epoch: 13/100 | Best Valid Score Until Now: 1.333 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 14/100 | Training Loss: 1.215 | Valid Score: 1.328\n",
            " \n",
            "Epoch: 14/100 | Best Valid Score Until Now: 1.328 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 15/100 | Training Loss: 1.200 | Valid Score: 1.346\n",
            " \n",
            "Epoch: 15/100 | Best Valid Score Until Now: 1.328 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 16/100 | Training Loss: 1.174 | Valid Score: 1.326\n",
            " \n",
            "Epoch: 16/100 | Best Valid Score Until Now: 1.326 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 17/100 | Training Loss: 1.155 | Valid Score: 1.325\n",
            " \n",
            "Epoch: 17/100 | Best Valid Score Until Now: 1.325 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 18/100 | Training Loss: 1.144 | Valid Score: 1.323\n",
            " \n",
            "Epoch: 18/100 | Best Valid Score Until Now: 1.323 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 19/100 | Training Loss: 1.124 | Valid Score: 1.315\n",
            " \n",
            "Epoch: 19/100 | Best Valid Score Until Now: 1.315 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 20/100 | Training Loss: 1.080 | Valid Score: 1.320\n",
            " \n",
            "Epoch: 20/100 | Best Valid Score Until Now: 1.315 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 21/100 | Training Loss: 1.061 | Valid Score: 1.300\n",
            " \n",
            "Epoch: 21/100 | Best Valid Score Until Now: 1.300 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 22/100 | Training Loss: 1.019 | Valid Score: 1.315\n",
            " \n",
            "Epoch: 22/100 | Best Valid Score Until Now: 1.300 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 23/100 | Training Loss: 0.992 | Valid Score: 1.298\n",
            " \n",
            "Epoch: 23/100 | Best Valid Score Until Now: 1.298 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 24/100 | Training Loss: 0.992 | Valid Score: 1.306\n",
            " \n",
            "Epoch: 24/100 | Best Valid Score Until Now: 1.298 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 25/100 | Training Loss: 1.002 | Valid Score: 1.298\n",
            " \n",
            "Epoch: 25/100 | Best Valid Score Until Now: 1.298 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 26/100 | Training Loss: 0.934 | Valid Score: 1.279\n",
            " \n",
            "Epoch: 26/100 | Best Valid Score Until Now: 1.279 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 27/100 | Training Loss: 0.911 | Valid Score: 1.283\n",
            " \n",
            "Epoch: 27/100 | Best Valid Score Until Now: 1.279 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 28/100 | Training Loss: 0.904 | Valid Score: 1.281\n",
            " \n",
            "Epoch: 28/100 | Best Valid Score Until Now: 1.279 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 29/100 | Training Loss: 0.875 | Valid Score: 1.284\n",
            " \n",
            "Epoch: 29/100 | Best Valid Score Until Now: 1.279 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 30/100 | Training Loss: 0.865 | Valid Score: 1.272\n",
            " \n",
            "Epoch: 30/100 | Best Valid Score Until Now: 1.272 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 31/100 | Training Loss: 0.851 | Valid Score: 1.317\n",
            " \n",
            "Epoch: 31/100 | Best Valid Score Until Now: 1.272 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 32/100 | Training Loss: 0.846 | Valid Score: 1.274\n",
            " \n",
            "Epoch: 32/100 | Best Valid Score Until Now: 1.272 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 33/100 | Training Loss: 0.832 | Valid Score: 1.324\n",
            " \n",
            "Epoch: 33/100 | Best Valid Score Until Now: 1.272 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 34/100 | Training Loss: 0.829 | Valid Score: 1.273\n",
            " \n",
            "Epoch: 34/100 | Best Valid Score Until Now: 1.272 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 35/100 | Training Loss: 0.832 | Valid Score: 1.254\n",
            " \n",
            "Epoch: 35/100 | Best Valid Score Until Now: 1.254 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 36/100 | Training Loss: 0.800 | Valid Score: 1.260\n",
            " \n",
            "Epoch: 36/100 | Best Valid Score Until Now: 1.254 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 37/100 | Training Loss: 0.803 | Valid Score: 1.260\n",
            " \n",
            "Epoch: 37/100 | Best Valid Score Until Now: 1.254 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 38/100 | Training Loss: 0.783 | Valid Score: 1.268\n",
            " \n",
            "Epoch: 38/100 | Best Valid Score Until Now: 1.254 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 39/100 | Training Loss: 0.783 | Valid Score: 1.262\n",
            " \n",
            "Epoch: 39/100 | Best Valid Score Until Now: 1.254 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 40/100 | Training Loss: 0.802 | Valid Score: 1.257\n",
            " \n",
            "Epoch: 40/100 | Best Valid Score Until Now: 1.254 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 41/100 | Training Loss: 0.750 | Valid Score: 1.292\n",
            " \n",
            "Epoch: 41/100 | Best Valid Score Until Now: 1.254 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 42/100 | Training Loss: 0.766 | Valid Score: 1.247\n",
            " \n",
            "Epoch: 42/100 | Best Valid Score Until Now: 1.247 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 43/100 | Training Loss: 0.737 | Valid Score: 1.261\n",
            " \n",
            "Epoch: 43/100 | Best Valid Score Until Now: 1.247 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 44/100 | Training Loss: 0.736 | Valid Score: 1.269\n",
            " \n",
            "Epoch: 44/100 | Best Valid Score Until Now: 1.247 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 45/100 | Training Loss: 0.729 | Valid Score: 1.234\n",
            " \n",
            "Epoch: 45/100 | Best Valid Score Until Now: 1.234 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 46/100 | Training Loss: 0.725 | Valid Score: 1.247\n",
            " \n",
            "Epoch: 46/100 | Best Valid Score Until Now: 1.234 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 47/100 | Training Loss: 0.736 | Valid Score: 1.303\n",
            " \n",
            "Epoch: 47/100 | Best Valid Score Until Now: 1.234 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 48/100 | Training Loss: 0.735 | Valid Score: 1.240\n",
            " \n",
            "Epoch: 48/100 | Best Valid Score Until Now: 1.234 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 49/100 | Training Loss: 0.698 | Valid Score: 1.254\n",
            " \n",
            "Epoch: 49/100 | Best Valid Score Until Now: 1.234 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 50/100 | Training Loss: 0.710 | Valid Score: 1.220\n",
            " \n",
            "Epoch: 50/100 | Best Valid Score Until Now: 1.220 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 51/100 | Training Loss: 0.706 | Valid Score: 1.243\n",
            " \n",
            "Epoch: 51/100 | Best Valid Score Until Now: 1.220 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 52/100 | Training Loss: 0.680 | Valid Score: 1.253\n",
            " \n",
            "Epoch: 52/100 | Best Valid Score Until Now: 1.220 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 53/100 | Training Loss: 0.683 | Valid Score: 1.252\n",
            " \n",
            "Epoch: 53/100 | Best Valid Score Until Now: 1.220 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 54/100 | Training Loss: 0.701 | Valid Score: 1.305\n",
            " \n",
            "Epoch: 54/100 | Best Valid Score Until Now: 1.220 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 55/100 | Training Loss: 0.678 | Valid Score: 1.255\n",
            " \n",
            "Epoch: 55/100 | Best Valid Score Until Now: 1.220 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 56/100 | Training Loss: 0.664 | Valid Score: 1.223\n",
            " \n",
            "Epoch: 56/100 | Best Valid Score Until Now: 1.220 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 57/100 | Training Loss: 0.668 | Valid Score: 1.240\n",
            " \n",
            "Epoch: 57/100 | Best Valid Score Until Now: 1.220 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 58/100 | Training Loss: 0.659 | Valid Score: 1.239\n",
            " \n",
            "Epoch: 58/100 | Best Valid Score Until Now: 1.220 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 59/100 | Training Loss: 0.663 | Valid Score: 1.222\n",
            " \n",
            "Epoch: 59/100 | Best Valid Score Until Now: 1.220 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 60/100 | Training Loss: 0.649 | Valid Score: 1.212\n",
            " \n",
            "Epoch: 60/100 | Best Valid Score Until Now: 1.212 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 61/100 | Training Loss: 0.660 | Valid Score: 1.289\n",
            " \n",
            "Epoch: 61/100 | Best Valid Score Until Now: 1.212 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 62/100 | Training Loss: 0.639 | Valid Score: 1.226\n",
            " \n",
            "Epoch: 62/100 | Best Valid Score Until Now: 1.212 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 63/100 | Training Loss: 0.625 | Valid Score: 1.219\n",
            " \n",
            "Epoch: 63/100 | Best Valid Score Until Now: 1.212 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 64/100 | Training Loss: 0.679 | Valid Score: 1.248\n",
            " \n",
            "Epoch: 64/100 | Best Valid Score Until Now: 1.212 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 65/100 | Training Loss: 0.668 | Valid Score: 1.205\n",
            " \n",
            "Epoch: 65/100 | Best Valid Score Until Now: 1.205 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 66/100 | Training Loss: 0.655 | Valid Score: 1.214\n",
            " \n",
            "Epoch: 66/100 | Best Valid Score Until Now: 1.205 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 67/100 | Training Loss: 0.615 | Valid Score: 1.218\n",
            " \n",
            "Epoch: 67/100 | Best Valid Score Until Now: 1.205 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 68/100 | Training Loss: 0.706 | Valid Score: 1.221\n",
            " \n",
            "Epoch: 68/100 | Best Valid Score Until Now: 1.205 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 69/100 | Training Loss: 0.638 | Valid Score: 1.200\n",
            " \n",
            "Epoch: 69/100 | Best Valid Score Until Now: 1.200 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 70/100 | Training Loss: 0.633 | Valid Score: 1.256\n",
            " \n",
            "Epoch: 70/100 | Best Valid Score Until Now: 1.200 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 71/100 | Training Loss: 0.607 | Valid Score: 1.237\n",
            " \n",
            "Epoch: 71/100 | Best Valid Score Until Now: 1.200 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 72/100 | Training Loss: 0.625 | Valid Score: 1.213\n",
            " \n",
            "Epoch: 72/100 | Best Valid Score Until Now: 1.200 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 73/100 | Training Loss: 0.600 | Valid Score: 1.214\n",
            " \n",
            "Epoch: 73/100 | Best Valid Score Until Now: 1.200 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 74/100 | Training Loss: 0.592 | Valid Score: 1.201\n",
            " \n",
            "Epoch: 74/100 | Best Valid Score Until Now: 1.200 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 75/100 | Training Loss: 0.581 | Valid Score: 1.215\n",
            " \n",
            "Epoch: 75/100 | Best Valid Score Until Now: 1.200 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 76/100 | Training Loss: 0.581 | Valid Score: 1.205\n",
            " \n",
            "Epoch: 76/100 | Best Valid Score Until Now: 1.200 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 77/100 | Training Loss: 0.576 | Valid Score: 1.219\n",
            " \n",
            "Epoch: 77/100 | Best Valid Score Until Now: 1.200 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 78/100 | Training Loss: 0.606 | Valid Score: 1.200\n",
            " \n",
            "Epoch: 78/100 | Best Valid Score Until Now: 1.200 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 79/100 | Training Loss: 0.586 | Valid Score: 1.232\n",
            " \n",
            "Epoch: 79/100 | Best Valid Score Until Now: 1.200 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 80/100 | Training Loss: 0.631 | Valid Score: 1.192\n",
            " \n",
            "Epoch: 80/100 | Best Valid Score Until Now: 1.192 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 81/100 | Training Loss: 0.568 | Valid Score: 1.209\n",
            " \n",
            "Epoch: 81/100 | Best Valid Score Until Now: 1.192 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 82/100 | Training Loss: 0.549 | Valid Score: 1.192\n",
            " \n",
            "Epoch: 82/100 | Best Valid Score Until Now: 1.192 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 83/100 | Training Loss: 0.561 | Valid Score: 1.228\n",
            " \n",
            "Epoch: 83/100 | Best Valid Score Until Now: 1.192 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 84/100 | Training Loss: 0.561 | Valid Score: 1.204\n",
            " \n",
            "Epoch: 84/100 | Best Valid Score Until Now: 1.192 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 85/100 | Training Loss: 0.543 | Valid Score: 1.238\n",
            " \n",
            "Epoch: 85/100 | Best Valid Score Until Now: 1.192 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 86/100 | Training Loss: 0.561 | Valid Score: 1.186\n",
            " \n",
            "Epoch: 86/100 | Best Valid Score Until Now: 1.186 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 87/100 | Training Loss: 0.535 | Valid Score: 1.243\n",
            " \n",
            "Epoch: 87/100 | Best Valid Score Until Now: 1.186 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 88/100 | Training Loss: 0.543 | Valid Score: 1.200\n",
            " \n",
            "Epoch: 88/100 | Best Valid Score Until Now: 1.186 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 89/100 | Training Loss: 0.525 | Valid Score: 1.204\n",
            " \n",
            "Epoch: 89/100 | Best Valid Score Until Now: 1.186 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 90/100 | Training Loss: 0.542 | Valid Score: 1.175\n",
            " \n",
            "Epoch: 90/100 | Best Valid Score Until Now: 1.175 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 91/100 | Training Loss: 0.527 | Valid Score: 1.190\n",
            " \n",
            "Epoch: 91/100 | Best Valid Score Until Now: 1.175 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 92/100 | Training Loss: 0.517 | Valid Score: 1.199\n",
            " \n",
            "Epoch: 92/100 | Best Valid Score Until Now: 1.175 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 93/100 | Training Loss: 0.523 | Valid Score: 1.193\n",
            " \n",
            "Epoch: 93/100 | Best Valid Score Until Now: 1.175 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 94/100 | Training Loss: 0.512 | Valid Score: 1.218\n",
            " \n",
            "Epoch: 94/100 | Best Valid Score Until Now: 1.175 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 95/100 | Training Loss: 0.512 | Valid Score: 1.177\n",
            " \n",
            "Epoch: 95/100 | Best Valid Score Until Now: 1.175 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 96/100 | Training Loss: 0.504 | Valid Score: 1.207\n",
            " \n",
            "Epoch: 96/100 | Best Valid Score Until Now: 1.175 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 97/100 | Training Loss: 0.515 | Valid Score: 1.217\n",
            " \n",
            "Epoch: 97/100 | Best Valid Score Until Now: 1.175 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 98/100 | Training Loss: 0.502 | Valid Score: 1.186\n",
            " \n",
            "Epoch: 98/100 | Best Valid Score Until Now: 1.175 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 99/100 | Training Loss: 0.503 | Valid Score: 1.223\n",
            " \n",
            "Epoch: 99/100 | Best Valid Score Until Now: 1.175 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 100/100 | Training Loss: 0.495 | Valid Score: 1.181\n",
            " \n",
            "Epoch: 100/100 | Best Valid Score Until Now: 1.175 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 1.175 \n",
            "\n",
            "Test Score: 1.168 \n",
            "\n",
            "Execution time: 276.825 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ]
    }
  ]
}